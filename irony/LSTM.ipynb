{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input ,Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
    "from common.attention import AttentionWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(fp):\n",
    "    '''\n",
    "    Loads the dataset .txt file with label-tweet on each line and parses the dataset.\n",
    "    :param fp: filepath of dataset\n",
    "    :return:\n",
    "        corpus: list of tweet strings of each tweet.\n",
    "        y: list of labels\n",
    "    '''\n",
    "    y = []\n",
    "    corpus = []\n",
    "    with open(fp, 'rt') as data_in:\n",
    "        for line in data_in:\n",
    "            if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "                line = line.rstrip() # remove trailing whitespace\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                corpus.append(tweet)\n",
    "\n",
    "    return corpus, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, labels = parse_dataset('datasets/train/SemEval2018-T3-train-taskA.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "maxlen=32\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from common.GloveEmbeddings import GloveEmbeddings\n",
    "embeddings = GloveEmbeddings(\n",
    "        '/media/radoslav/ce763dbf-b2a6-4110-960f-2ef10c8c6bde/MachineLearning/glove.twitter.27B/glove.twitter.27B.200d.txt',\n",
    "        200).load().get_embedding_matrix_for_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = tokenizer.texts_to_sequences(tweets)\n",
    "processed = pad_sequences(processed, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(embeddings.shape[0], embeddings.shape[1], weights=[embeddings], trainable=False))\n",
    "    model.add(Bidirectional(LSTM(10, dropout=0.3, return_sequences=True)))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         2584800   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 20)          16880     \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,602,953\n",
      "Trainable params: 18,089\n",
      "Non-trainable params: 2,584,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed, labels, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2875, 32)\n",
      "(959, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2875 samples, validate on 959 samples\n",
      "Epoch 1/100\n",
      "2875/2875 [==============================] - 2s 535us/step - loss: 0.7421 - acc: 0.5332 - val_loss: 0.6913 - val_acc: 0.5057\n",
      "Epoch 2/100\n",
      "2875/2875 [==============================] - 1s 228us/step - loss: 0.7074 - acc: 0.5551 - val_loss: 0.6904 - val_acc: 0.5130\n",
      "Epoch 3/100\n",
      "2875/2875 [==============================] - 1s 232us/step - loss: 0.7010 - acc: 0.5603 - val_loss: 0.6897 - val_acc: 0.4953\n",
      "Epoch 4/100\n",
      "2875/2875 [==============================] - 1s 230us/step - loss: 0.6824 - acc: 0.5690 - val_loss: 0.6892 - val_acc: 0.4953\n",
      "Epoch 5/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.6824 - acc: 0.5736 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 6/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6769 - acc: 0.5781 - val_loss: 0.6885 - val_acc: 0.4943\n",
      "Epoch 7/100\n",
      "2875/2875 [==============================] - 1s 230us/step - loss: 0.6793 - acc: 0.5819 - val_loss: 0.6882 - val_acc: 0.4932\n",
      "Epoch 8/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.6755 - acc: 0.5875 - val_loss: 0.6870 - val_acc: 0.5391\n",
      "Epoch 9/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6589 - acc: 0.6063 - val_loss: 0.6860 - val_acc: 0.5610\n",
      "Epoch 10/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.6632 - acc: 0.6017 - val_loss: 0.6850 - val_acc: 0.5912\n",
      "Epoch 11/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.6655 - acc: 0.5969 - val_loss: 0.6843 - val_acc: 0.5850\n",
      "Epoch 12/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.6563 - acc: 0.6132 - val_loss: 0.6841 - val_acc: 0.5610\n",
      "Epoch 13/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6530 - acc: 0.6157 - val_loss: 0.6834 - val_acc: 0.5631\n",
      "Epoch 14/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.6448 - acc: 0.6268 - val_loss: 0.6812 - val_acc: 0.6058\n",
      "Epoch 15/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6452 - acc: 0.6223 - val_loss: 0.6794 - val_acc: 0.6330\n",
      "Epoch 16/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6403 - acc: 0.6184 - val_loss: 0.6785 - val_acc: 0.6121\n",
      "Epoch 17/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.6368 - acc: 0.6369 - val_loss: 0.6763 - val_acc: 0.6413\n",
      "Epoch 18/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.6271 - acc: 0.6386 - val_loss: 0.6738 - val_acc: 0.6382\n",
      "Epoch 19/100\n",
      "2875/2875 [==============================] - 1s 232us/step - loss: 0.6359 - acc: 0.6459 - val_loss: 0.6711 - val_acc: 0.6309\n",
      "Epoch 20/100\n",
      "2875/2875 [==============================] - 1s 238us/step - loss: 0.6230 - acc: 0.6595 - val_loss: 0.6697 - val_acc: 0.6403\n",
      "Epoch 21/100\n",
      "2875/2875 [==============================] - 1s 234us/step - loss: 0.6242 - acc: 0.6609 - val_loss: 0.6677 - val_acc: 0.6413\n",
      "Epoch 22/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.6160 - acc: 0.6640 - val_loss: 0.6669 - val_acc: 0.6298\n",
      "Epoch 23/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.6092 - acc: 0.6619 - val_loss: 0.6631 - val_acc: 0.6330\n",
      "Epoch 24/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.6069 - acc: 0.6664 - val_loss: 0.6594 - val_acc: 0.6246\n",
      "Epoch 25/100\n",
      "2875/2875 [==============================] - 1s 233us/step - loss: 0.6057 - acc: 0.6748 - val_loss: 0.6569 - val_acc: 0.6423\n",
      "Epoch 26/100\n",
      "2875/2875 [==============================] - 1s 240us/step - loss: 0.6033 - acc: 0.6730 - val_loss: 0.6555 - val_acc: 0.6371\n",
      "Epoch 27/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.6077 - acc: 0.6633 - val_loss: 0.6547 - val_acc: 0.6309\n",
      "Epoch 28/100\n",
      "2875/2875 [==============================] - 1s 242us/step - loss: 0.5937 - acc: 0.6814 - val_loss: 0.6512 - val_acc: 0.6413\n",
      "Epoch 29/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.5913 - acc: 0.6828 - val_loss: 0.6489 - val_acc: 0.6455\n",
      "Epoch 30/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.5916 - acc: 0.6918 - val_loss: 0.6437 - val_acc: 0.6371\n",
      "Epoch 31/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.5791 - acc: 0.6849 - val_loss: 0.6400 - val_acc: 0.6486\n",
      "Epoch 32/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.5876 - acc: 0.6856 - val_loss: 0.6400 - val_acc: 0.6403\n",
      "Epoch 33/100\n",
      "2875/2875 [==============================] - 1s 231us/step - loss: 0.5819 - acc: 0.6817 - val_loss: 0.6372 - val_acc: 0.6434\n",
      "Epoch 34/100\n",
      "2875/2875 [==============================] - 1s 243us/step - loss: 0.5630 - acc: 0.7023 - val_loss: 0.6321 - val_acc: 0.6486\n",
      "Epoch 35/100\n",
      "2875/2875 [==============================] - 1s 241us/step - loss: 0.5741 - acc: 0.6970 - val_loss: 0.6294 - val_acc: 0.6423\n",
      "Epoch 36/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.5623 - acc: 0.7030 - val_loss: 0.6280 - val_acc: 0.6486\n",
      "Epoch 37/100\n",
      "2875/2875 [==============================] - 1s 242us/step - loss: 0.5608 - acc: 0.7092 - val_loss: 0.6253 - val_acc: 0.6434\n",
      "Epoch 38/100\n",
      "2875/2875 [==============================] - 1s 237us/step - loss: 0.5508 - acc: 0.7190 - val_loss: 0.6231 - val_acc: 0.6434\n",
      "Epoch 39/100\n",
      "2875/2875 [==============================] - 1s 232us/step - loss: 0.5452 - acc: 0.7203 - val_loss: 0.6208 - val_acc: 0.6538\n",
      "Epoch 40/100\n",
      "2875/2875 [==============================] - 1s 244us/step - loss: 0.5410 - acc: 0.7144 - val_loss: 0.6223 - val_acc: 0.6413\n",
      "Epoch 41/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.5482 - acc: 0.7155 - val_loss: 0.6194 - val_acc: 0.6538\n",
      "Epoch 42/100\n",
      "2875/2875 [==============================] - 1s 239us/step - loss: 0.5351 - acc: 0.7287 - val_loss: 0.6184 - val_acc: 0.6517\n",
      "Epoch 43/100\n",
      "2875/2875 [==============================] - 1s 240us/step - loss: 0.5288 - acc: 0.7357 - val_loss: 0.6174 - val_acc: 0.6507\n",
      "Epoch 44/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.5271 - acc: 0.7339 - val_loss: 0.6192 - val_acc: 0.6434\n",
      "Epoch 45/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.5223 - acc: 0.7339 - val_loss: 0.6207 - val_acc: 0.6548\n",
      "Epoch 46/100\n",
      "2875/2875 [==============================] - 1s 235us/step - loss: 0.5268 - acc: 0.7384 - val_loss: 0.6191 - val_acc: 0.6496\n",
      "Epoch 47/100\n",
      "2875/2875 [==============================] - 1s 240us/step - loss: 0.5223 - acc: 0.7377 - val_loss: 0.6230 - val_acc: 0.6496\n",
      "Epoch 48/100\n",
      "2875/2875 [==============================] - 1s 236us/step - loss: 0.5119 - acc: 0.7419 - val_loss: 0.6253 - val_acc: 0.6580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f9c572748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlyStopping = EarlyStopping(patience=5)\n",
    "model.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size=256, epochs=100, callbacks=[earlyStopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.71      0.67       474\n",
      "          1       0.68      0.60      0.64       485\n",
      "\n",
      "avg / total       0.66      0.66      0.66       959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
